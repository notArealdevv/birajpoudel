{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notArealdevv/birajpoudel/blob/main/Customer_Churn_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Customer Churn Prediction Model\n",
        "# This script replicates the project described in the resume, demonstrating how to build a\n",
        "# classification model to predict customer churn for a telecom company.\n",
        "\n",
        "#\n",
        "# Technical Stack: Python, Pandas, Scikit-learn\n",
        "#\n",
        "\n",
        "# --- 1. Import Necessary Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "# To handle imbalanced classes, we can use a library like imbalanced-learn\n",
        "# You may need to install it: pip install -U imbalanced-learn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# --- 2. Load and Prepare Sample Data ---\n",
        "# In a real-world scenario, this data would be loaded from a CSV or a database.\n",
        "# For this example, we'll use a small, representative sample of a telecom dataset.\n",
        "# The data includes customer demographics, account information, and whether they churned.\n",
        "\n",
        "csv_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
        "7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No\n",
        "5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No\n",
        "3668-QPYBK,Male,0,No,No,2,Yes,No,DSL,Yes,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,53.85,108.15,Yes\n",
        "7795-CFOCW,Male,0,No,No,45,No,No phone service,DSL,Yes,No,Yes,Yes,No,No,One year,No,Bank transfer (automatic),42.3,1840.75,No\n",
        "9237-HQITU,Female,0,No,No,2,Yes,No,Fiber optic,No,No,No,No,No,No,Month-to-month,Yes,Electronic check,70.7,151.65,Yes\n",
        "1452-KIOVK,Male,0,No,Yes,22,Yes,Yes,Fiber optic,No,Yes,No,No,Yes,No,Month-to-month,Yes,Credit card (automatic),89.1,1949.4,No\n",
        "6713-OKOMC,Female,0,No,No,10,Yes,No,DSL,Yes,No,No,No,No,No,Month-to-month,No,Mailed check,29.75,301.9,No\n",
        "8779-SDGTV,Male,0,No,No,2,Yes,No,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,50.8,102.45,Yes\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(StringIO(csv_data))\n",
        "\n",
        "# Data Cleaning: Convert TotalCharges to numeric, filling missing values\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop(['customerID', 'Churn'], axis=1)\n",
        "y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# --- 3. Feature Engineering and Preprocessing ---\n",
        "# We need to handle categorical and numerical features differently.\n",
        "# - Numerical features will be scaled.\n",
        "# - Categorical features will be one-hot encoded.\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# --- 4. Handling Unbalanced Classes ---\n",
        "# Churn datasets are often imbalanced. We'll use RandomOverSampler to balance the classes.\n",
        "# This technique creates synthetic samples for the minority class.\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "\n",
        "# Split the balanced data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# --- 5. Model Training and Hyperparameter Tuning ---\n",
        "# As per the resume, we will build and compare Logistic Regression and Random Forest models.\n",
        "# We'll use GridSearchCV to find the best hyperparameters.\n",
        "\n",
        "# --- Model 1: Logistic Regression ---\n",
        "print(\"--- Training Logistic Regression Model ---\")\n",
        "# Create the full pipeline including preprocessing and the model\n",
        "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', LogisticRegression(solver='liblinear', random_state=42))])\n",
        "\n",
        "# Define hyperparameters to search\n",
        "lr_param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'classifier__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "lr_grid_search = GridSearchCV(lr_pipeline, lr_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_lr = lr_grid_search.best_estimator_\n",
        "print(f\"Best Logistic Regression Parameters: {lr_grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "# --- Model 2: Random Forest ---\n",
        "print(\"\\n--- Training Random Forest Model ---\")\n",
        "# Create the full pipeline\n",
        "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "# Define hyperparameters to search\n",
        "rf_param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = rf_grid_search.best_estimator_\n",
        "print(f\"Best Random Forest Parameters: {rf_grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "# --- 6. Model Evaluation ---\n",
        "print(\"\\n--- Evaluating Models on Test Data ---\")\n",
        "# Evaluate Logistic Regression\n",
        "lr_predictions = best_lr.predict(X_test)\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_predictions):.4f}\")\n",
        "\n",
        "# Evaluate Random Forest\n",
        "rf_predictions = best_rf.predict(X_test)\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_predictions):.4f}\")\n",
        "\n",
        "print(\"\\n--- Project Conclusion ---\")\n",
        "print(\"Both models were trained and evaluated successfully.\")\n",
        "print(\"The Random Forest Classifier generally provides better performance due to its ability to capture non-linear relationships.\")\n",
        "print(\"The process demonstrated data preprocessing, handling class imbalance, hyperparameter tuning, and model evaluation.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "oJD_dfY1e79p"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}